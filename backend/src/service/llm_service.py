"""
llm_paper_tools.py

æä¾›ï¼š
- å•è½® LLM è°ƒç”¨
- å¤šè½® chat è°ƒç”¨
- æ‘˜è¦ç¿»è¯‘
- é•¿æ–‡è‡ªåŠ¨ chunk + æ€»ç»“
- PaperChatStateï¼šå¸¦æ‘˜è¦+æ€»ç»“+å†å²å¯¹è¯çš„é—®ç­”

ä¾èµ–ï¼š
    pip install litellm
"""

from __future__ import annotations
from dataclasses import dataclass, field
from typing import List, Dict

import litellm
from litellm import completion
from ..config import Config

def init_litellm():
    litellm.api_key = Config.chat_litellm.api_key
    litellm.api_base = Config.chat_litellm.api_base

# =========================================================
# ğŸ”¹ 1. åŸºç¡€ LLM å°è£…
# =========================================================

def llm_completion(prompt: str) -> str:
    """å•è½®å¯¹è¯"""
    resp = completion(
        model=Config.chat_litellm.model,
        messages=[{"role": "user", "content": prompt}],
    )
    return resp.choices[0].message.content


def llm_chat(messages: List[Dict]) -> str:
    """å¤šè½®å¯¹è¯"""
    resp = completion(
        model=Config.chat_litellm.model,
        messages=messages,
    )
    return resp.choices[0].message.content


def translate_title(title_text: str, target_lang: str = "zh") -> str:
    prompt = f"""
ä½ æ˜¯ä¸€åä¸¥è°¨çš„å­¦æœ¯ç¿»è¯‘åŠ©æ‰‹ï¼Œè¯·å°†ä¸‹é¢çš„å­¦æœ¯æ ‡é¢˜ç¿»è¯‘æˆ {target_lang}ï¼Œè¦æ±‚ï¼š
- ä¿æŒæœ¯è¯­å‡†ç¡®
- ä¸è¦æ·»åŠ ä¸ªäººè¯„è®º

åŸæ–‡æ ‡é¢˜ï¼š
{title_text}
"""
    return llm_completion(prompt.strip())


# =========================================================
# ğŸ”¹ 2. æ‘˜è¦ç¿»è¯‘
# =========================================================

def translate_summary(summary_text: str, target_lang: str = "zh") -> str:
    prompt = f"""
ä½ æ˜¯ä¸€åä¸¥è°¨çš„å­¦æœ¯ç¿»è¯‘åŠ©æ‰‹ï¼Œè¯·å°†ä¸‹é¢çš„å­¦æœ¯æ‘˜è¦ç¿»è¯‘æˆ {target_lang}ï¼Œè¦æ±‚ï¼š
- ä¿æŒæœ¯è¯­å‡†ç¡®
- æ•°å­¦ç¬¦å·ä¿æŒä¸å˜
- ä¸è¦æ·»åŠ ä¸ªäººè¯„è®º

åŸæ–‡æ‘˜è¦ï¼š
{summary_text}
"""
    return llm_completion(prompt.strip())


# =========================================================
# ğŸ”¹ 3. é•¿æ–‡å¤„ç† â€”â€” åˆ‡ chunk + åˆ†å—æ€»ç»“ + æ€»æ€»ç»“
# =========================================================

def _split_text(text: str, max_chars: int = 6000) -> List[str]:
    """
    æŒ‰å­—ç¬¦é•¿åº¦+æ®µè½åˆ‡å—ï¼Œç²—ç•¥ proxy tokenã€‚
    6000 å­—ç¬¦ ~ 1500â€“2000 token
    """
    paragraphs = text.split("\n\n")
    chunks, buf = [], []
    buf_len = 0

    for p in paragraphs:
        p = p.strip()
        if not p:
            continue

        if buf_len + len(p) + 2 <= max_chars:
            buf.append(p)
            buf_len += len(p) + 2
        else:
            if buf:
                chunks.append("\n\n".join(buf))
            buf = [p]
            buf_len = len(p)

    if buf:
        chunks.append("\n\n".join(buf))

    return chunks


def summarize_long_markdown(md_text: str, language: str = "en") -> str:
    """
    è‡ªåŠ¨ chunk + åˆå¹¶æ€»ç»“
    """
    chunks = [c for c in _split_text(md_text, max_chars=6000) if c.strip()]

    if not chunks:
        return ""

    # === åªæœ‰ 1 å—ï¼Œç›´æ¥æ€»ç»“ ===
    if len(chunks) == 1:
        prompt = f"""
You are an expert academic assistant. Please summarize the following paper content in {language}.

Requirements:
- Describe: problem â†’ motivation â†’ method â†’ theory(if any) â†’ experiments â†’ conclusions
- 3â€“6 paragraphs
- Avoid copying sentences verbatim

Content:
{chunks[0]}
"""
        return llm_completion(prompt.strip())

    # === å¤šå—æ€»ç»“ ===
    partial_summaries = []

    for i, chunk in enumerate(chunks, start=1):
        prompt = f"""
You are an expert academic assistant. This is part {i}/{len(chunks)} of a paper.
Summarize ONLY this part in {language}.

Requirements:
- 1â€“3 paragraphs
- Keep technical precision
- Do not guess other sections

Content:
{chunk}
"""
        summary = llm_completion(prompt.strip())
        partial_summaries.append(summary)

    # === åˆå¹¶ summaries ===
    joined = "\n\n---\n\n".join(partial_summaries)

    final_prompt = f"""
You are an expert academic assistant.

Below are partial summaries of a paper.
Merge them into ONE coherent full-paper summary in {language}.

Requirements:
- Structure: problem â†’ motivation â†’ method â†’ theory(if any) â†’ experiments â†’ conclusion/limitations
- 3â€“7 paragraphs
- Avoid redundancy and contradictions
- Do NOT invent content beyond these summaries

Partial summaries:
{joined}
"""
    return llm_completion(final_prompt.strip())


# =========================================================
# ğŸ”¹ 4. Paper Chat çŠ¶æ€ â€”â€” Sidebar é—®ç­”æ”¯æŒ
# =========================================================

@dataclass
class PaperChatState:
    """
    ç»´æŠ¤ï¼š
    - æ ‡é¢˜
    - æ‘˜è¦ï¼ˆå¯ç¿»è¯‘åï¼‰
    - å…¨æ–‡æ€»ç»“
    - å†å²é—®ç­”
    """
    paper_title: str
    paper_abstract: str
    paper_full_summary: str
    history: List[Dict] = field(default_factory=list)


def ask_paper_question(
    state: PaperChatState,
    question: str,
    language: str = "en",
) -> str:
    """
    ç”¨äº sidebar é—®ç­”ï¼š
    - å°†æ‘˜è¦ + æ€»ç»“ + å†å²ä½œä¸ºä¸Šä¸‹æ–‡
    - ä¸¥æ ¼é¿å…å¹»è§‰
    """

    system_prompt = f"""
You are an expert assistant helping interpret a research paper.

Paper Title:
{state.paper_title}

Abstract:
{state.paper_abstract}

Full Summary:
{state.paper_full_summary}

Rules:
- Answer ONLY using the above paper info + chat history
- If the answer is not implied, say you don't know
- Respond in {language}
"""

    messages = [{"role": "system", "content": system_prompt.strip()}]
    messages.extend(state.history)
    messages.append({"role": "user", "content": question})

    answer = llm_chat(messages)

    # è¿½åŠ å†å²
    state.history.append({"role": "user", "content": question})
    state.history.append({"role": "assistant", "content": answer})

    return answer